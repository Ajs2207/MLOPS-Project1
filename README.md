# ğŸš€ Vehicle Data MLOps Project - End-to-End Machine Learning Pipeline

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![MLOps](https://img.shields.io/badge/MLOps-Platform-orange.svg)](https://ml-ops.org/)
[![Docker](https://img.shields.io/badge/Docker-Container-blue.svg)](https://www.docker.com/)
[![AWS](https://img.shields.io/badge/AWS-Cloud-yellow.svg)](https://aws.amazon.com/)
[![MongoDB](https://img.shields.io/badge/MongoDB-Atlas-green.svg)](https://www.mongodb.com/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-Github%20Actions-blue.svg)](https://github.com/features/actions)

## ğŸ“‹ Table of Contents
- [âœ¨ Overview](#-overview)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Features](#-features)
- [ğŸ“ Project Structure](#-project-structure)
- [âš™ï¸ Installation & Setup](#ï¸-installation--setup)
- [ğŸ”§ Pipeline Components](#-pipeline-components)
- [ğŸŒ Deployment](#-deployment)
- [ğŸ“Š Monitoring & Logging](#-monitoring--logging)
- [ğŸ¤ Contributing](#-contributing)

## âœ¨ Overview

An enterprise-grade MLOps platform for vehicle data analysis and predictive modeling. This project demonstrates a complete machine learning lifecycle from data ingestion to production deployment, featuring automated CI/CD pipelines, cloud infrastructure, and comprehensive monitoring.

**Key Highlights:**
- ğŸ”„ **End-to-End ML Pipeline**: From raw data to production-ready API
- â˜ï¸ **Cloud-Native Architecture**: Leveraging AWS, MongoDB Atlas, and Docker
- ğŸ¤– **Automated CI/CD**: GitHub Actions for seamless deployment
- ğŸ“ˆ **Production-Ready**: Scalable, maintainable, and monitored

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[MongoDB Atlas] --> B[Data Ingestion]
    B --> C[Data Validation]
    C --> D[Data Transformation]
    D --> E[Model Training]
    E --> F[Model Evaluation]
    F --> G[Model Registry<br/>AWS S3]
    G --> H[Model Deployment]
    H --> I[Flask API<br/>EC2 Instance]
    J[GitHub Actions] --> K[CI/CD Pipeline]
    K --> L[Docker Container<br/>ECR]
    L --> I
```

## ğŸ› ï¸ Tech Stack

### **Backend & ML**
- ![Python](https://img.shields.io/badge/-Python-3776AB?logo=python&logoColor=white) Python 3.10
- ![Scikit-learn](https://img.shields.io/badge/-Scikit--learn-F7931E?logo=scikit-learn&logoColor=white) Scikit-learn
- ![Pandas](https://img.shields.io/badge/-Pandas-150458?logo=pandas&logoColor=white) Pandas & NumPy
- ![Flask](https://img.shields.io/badge/-Flask-000000?logo=flask&logoColor=white) Flask API

### **Database & Storage**
- ![MongoDB](https://img.shields.io/badge/-MongoDB-47A248?logo=mongodb&logoColor=white) MongoDB Atlas
- ![AWS S3](https://img.shields.io/badge/-AWS%20S3-569A31?logo=amazons3&logoColor=white) AWS S3 Buckets

### **Cloud & DevOps**
- ![AWS](https://img.shields.io/badge/-AWS-232F3E?logo=amazonaws&logoColor=white) AWS EC2, ECR, IAM
- ![Docker](https://img.shields.io/badge/-Docker-2496ED?logo=docker&logoColor=white) Docker Containerization
- ![GitHub Actions](https://img.shields.io/badge/-GitHub%20Actions-2088FF?logo=githubactions&logoColor=white) GitHub Actions CI/CD

### **Monitoring & Quality**
- Custom Logging & Exception Handling
- Data Validation with Schema Management
- Model Performance Tracking

## ğŸš€ Features

### **ğŸ”§ Core ML Pipeline**
- **Automated Data Ingestion**: Real-time data pipeline from MongoDB Atlas
- **Smart Data Validation**: Schema-based validation with configurable rules
- **Feature Engineering**: Automated transformation pipelines
- **Model Versioning**: Track and compare model performance
- **A/B Testing**: Model evaluation with configurable thresholds

### **â˜ï¸ Cloud Integration**
- **Secure Credential Management**: Environment-based configuration
- **Scalable Storage**: AWS S3 for model artifact storage
- **Containerized Deployment**: Docker-based consistent environments
- **Auto-scaling Ready**: EC2 instance with configurable specifications

### **ğŸ”„ DevOps Excellence**
- **One-Click Deployment**: Automated CI/CD with GitHub Actions
- **Self-hosted Runners**: Dedicated EC2 instances for pipeline execution
- **Blue-Green Deployment**: Safe rollouts with zero downtime
- **Infrastructure as Code**: Reproducible cloud setup

### **ğŸ“Š Monitoring & Operations**
- **Centralized Logging**: Structured logs for debugging and auditing
- **Performance Metrics**: Track model drift and data quality
- **Health Checks**: Automated system monitoring
- **Alerting System**: Proactive issue detection

## ğŸ“ Project Structure

```
vehicle-mlops/
â”‚
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ components/               # Pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â””â”€â”€ model_pusher.py
â”‚   â”‚
â”‚   â”œâ”€â”€ configuration/            # Configuration management
â”‚   â”‚   â”œâ”€â”€ mongo_db_connection.py
â”‚   â”‚   â””â”€â”€ aws_connection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/                   # Data entities and schemas
â”‚   â”‚   â”œâ”€â”€ config_entity.py
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”‚   â””â”€â”€ estimator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ aws_storage/              # AWS S3 operations
â”‚   â”œâ”€â”€ constants/                # Project constants
â”‚   â”œâ”€â”€ data_access/              # Data layer
â”‚   â”œâ”€â”€ exception/                # Custom exceptions
â”‚   â”œâ”€â”€ logger/                   # Logging configuration
â”‚   â””â”€â”€ utils/                    # Utility functions
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ EDA.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ feature_engineering.ipynb
â”‚   â””â”€â”€ mongoDB_demo.ipynb       # MongoDB operations demo
â”‚
â”œâ”€â”€ templates/                    # Flask templates
â”œâ”€â”€ static/                       # Static assets
â”œâ”€â”€ artifact/                     # Pipeline artifacts
â”œâ”€â”€ tests/                        # Test suite
â”‚
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ config/                       # Configuration files
â”‚   â””â”€â”€ schema.yaml              # Data validation schema
â”‚
â”œâ”€â”€ Dockerfile                    # Docker configuration
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .github/workflows/           # CI/CD pipelines
â”‚   â””â”€â”€ aws.yaml
â”œâ”€â”€ app.py                       # Flask application
â””â”€â”€ setup.py                     # Package setup
```

## âš™ï¸ Installation & Setup

### **Prerequisites**
```bash
# System Requirements
- Python 3.10
- Conda/Miniconda
- Docker (for containerization)
- AWS Account (for deployment)
- MongoDB Atlas Account
```

### **Quick Start**

1. **Clone and Setup Environment**
```bash
# Create project from template
python template.py

# Create virtual environment
conda create -n vehicle python=3.10 -y
conda activate vehicle

# Install dependencies
pip install -r requirements.txt

# Verify installation
pip list
```

2. **Database Setup (MongoDB Atlas)**
```bash
# Set MongoDB connection string
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster.mongodb.net/"
```

3. **AWS Configuration**
```bash
# Set AWS credentials
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_DEFAULT_REGION="us-east-1"
```

## ğŸ”§ Pipeline Components

### **1. Data Ingestion**
- Connects to MongoDB Atlas
- Fetches vehicle data in key-value format
- Transforms to pandas DataFrame
- Stores raw data artifacts

### **2. Data Validation**
- Schema validation using YAML configuration
- Data type and range checking
- Missing value detection
- Data quality reports

### **3. Data Transformation**
- Feature engineering pipelines
- Encoding categorical variables
- Scaling numerical features
- Train-test split management

### **4. Model Training**
- Multiple algorithm support
- Hyperparameter tuning
- Cross-validation
- Model persistence

### **5. Model Evaluation**
- Performance metrics calculation
- Threshold-based model comparison
- AWS S3 integration for model registry
- Drift detection

### **6. Model Deployment**
- Flask REST API
- Docker containerization
- AWS EC2 deployment
- Load balancing ready

## ğŸŒ Deployment

### **CI/CD Pipeline**
```yaml
# GitHub Actions Workflow (.github/workflows/aws.yaml)
- Trigger: On push to main branch
- Steps:
  1. Code checkout
  2. Run tests
  3. Build Docker image
  4. Push to AWS ECR
  5. Deploy to EC2
  6. Health check
```

### **AWS Infrastructure**
- **EC2**: T2 Medium instance (Ubuntu 24.04)
- **S3**: Model artifact storage (`my-model-mlopsproj`)
- **ECR**: Docker image registry
- **IAM**: Secure access management

### **Deployment Commands**
```bash
# Manual deployment
docker build -t vehicle-mlops .
docker run -p 5000:5000 vehicle-mlops

# Access application
http://<ec2-public-ip>:5000
```

## ğŸ“Š Monitoring & Logging

### **Logging Features**
- Structured JSON logging
- Multi-level logging (DEBUG, INFO, ERROR)
- File and console handlers
- Rotating log files

### **Performance Tracking**
- Model accuracy metrics
- Inference latency
- Resource utilization
- Error rates

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

---

## ğŸ“ Contact & Support

For questions:
- ğŸ“§ Email: abhijeetsml4@gmail.com
- ğŸ’¼ LinkedIn: [linkedin.com/in/abhijeet-samal](https://www.linkedin.com/in/abhijeet-samal/)

---

**â­ Star this repo if you find it useful!**

---
*This project demonstrates professional MLOps practices suitable for production environments. Built with scalability, maintainability, and best practices in mind.*

---